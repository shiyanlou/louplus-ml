{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"https://doc.shiyanlou.com/document-uid214893labid7506timestamp1555400601684.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习完成假新闻分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"> 以下内容仅保留挑战参考答案代码部分，完整挑战请到原课程页面查看。</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc \"http://labfile.oss.aliyuncs.com/courses/1233/wsdm_mini.csv\"  # 假新闻数据\n",
    "!wget -nc \"http://labfile.oss.aliyuncs.com/courses/1176/stopwords.txt\"  # 停用词词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"wsdm_mini.csv\")\n",
    "df['title_zh'] = df[['title1_zh', 'title2_zh']].apply(\n",
    "    lambda x: ''.join(x), axis=1)  # 合并文本数据列\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        stopwords = [line.strip('\\n') for line in f.readlines()]\n",
    "    return stopwords\n",
    "\n",
    "stopwords = load_stopwords('stopwords.txt')\n",
    "\n",
    "corpus = []\n",
    "for line in tqdm_notebook(df['title_zh']):\n",
    "    words = []\n",
    "    seg_list = list(jieba.cut(line))  # 分词\n",
    "    for word in seg_list:\n",
    "        if word in stopwords:  # 删除停用词\n",
    "            continue\n",
    "        words.append(word)\n",
    "    corpus.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus)\n",
    "X_ = tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in X_[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.preprocessing.sequence.pad_sequences(X_, maxlen=20)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y_onehot = encoder.fit_transform(df.label.values.reshape(len(df), -1))\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(10000, 16, input_length=20))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, 64, 10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://doc.shiyanlou.com/document-uid214893labid7506timestamp1545810029884.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
